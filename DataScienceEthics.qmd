---
title: "Facial Recognition and Data Ethics Problems"
description: |
  A piece on the question of ethics for and the intented or unintended consequences of AI facial recognition being used commercially
author: Uras Uyal
date: April 16, 2025
format: html
execute: 
  warning: false
  message: false
---

# Background
Artificial Intelligence is used in every part of our daily lives now -- in circumstances in which we realize it and in those we do not. One way of rating or classifying people, often used ethically but also just as often used unethically, is via using their face, gender, and/or skin color data. Facial recognition software are used everywhere: from direct applications such as perhaps opening your phone with a Face ID or going through automatic passport check-in to modifiable applications such as your facial data being used in order to help admit you to a job or for governmental purposes. Even if the uses can be in good light and most facial classification software might try to use it in order to promote just and fair decisions, regardless, the data itself may still be unsuitable for such purposes without initial cleansing and bias considerations. 

According to Buolamwini and Gebru's study "Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification", skin and facial data is highly skewed towards those with lighter skin and significantly skewed towards those who are classified as male. Many data-users who do not realize this, or do not adjust their outcomes to this bias, even if they make their AI take decisions in good light, end up having decisions which strongly favor lighter skinned individuals or male ones. This can be either because of the abundance of those types of facial data being more abundant will make the algorithm more confident to choose them, or more importantly, since algorithms will be trained on past data, past usages per capita being more abundant for lighter skinned individuals or male individuals will make the algorithm think that it is more positive by nature to choose such individuals; amongst other reasons. This bias has huge significance in our modern day world where increasingly AI is making decisions instead of humans. Buolamwini and Gebru's study also highlights how misclassification of racial data for computer vision can be a reason for inaccuracies in fairness, for example for mixed-race or in-between skin colors individuals, highlighting that people groups' phenotypes can change over time and represents a broad spectrum.

Another study by Denise Almeida, Konstantin Shmarko, and Elizabeth Lomas compares the different actions for facial recognition software that is being taken in the UK, EU, and the US. They highlight that with COVID-19, facial recognition has multiplied in terms of importance. They state that for the EU and UK, although a facial recognition oriented privacy act does not exist, the EU's general data protection law, the General Data Protection Regulation (GDPR) protects local or foreign companies from using EU citizens' data in certain ways. A lot of these uses would normally be covered in a long terms of use elsewhere in the world and freely used. However, the authors still claim that these measures are not enough. On the other hand, according to the paper, a framework in the USA does not exist for facial recognition software data privacy; however, some California cities have banned the usage of any facial recognition technology and some companies like IBM and Amazon have promised and taken action to double down on not using facial recognition software for usages that may be harmed by inclusions of biases. However, when looked at the issue as a widespread problem, more actions should be taken all around the world in terms of data privacy as well. This raises questions in using such data in which where they come from or what kinds of biases they may include is or may be vague.

As it stands, the commercial usage of facial recognition software is a very important and relevant issue, as so many possibilities of unethical usage exist: most facial recognition data itself is skewed towards biasing males or lighter skinned people, and the sharing of this data and if there were biases inputted into the data by itself is vague. Even if someone wanted to use the data with good intentions, it is highly improbable that the data that they have is fully ethically sound.


Some questions or points that may come up as good discussion prompts for facial recognition AI and ethics and for Buolamwini and Gebru's study "Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification" are as follows:



# Who was measured? Are those individuals representative of the people to whom we’d like to generalize / apply the algorithm? Should we analyze data if we do not know how the data were collected?

Buolamwini and Gebru exactly highlight in their study that the individuals that are part of the data sets for most usages of facial recognition data are not representative of the people whom we'd like to generalize. The people whom we'd like to generalize would be the general public, with as low bias or none, if at all possible, towards any group that an individual could belong to. An ideal algorithm would look at the individual and not be biased by groups (ethnic, religious, sexual, etc.) that the individual may be part of. Instead, people who were measured were mostly lighter skinned or males. Furthermore, Denise Almeida, Konstantin Shmarko, and Elizabeth Lomas's also touch upon how there is a lack of transparency in big facial recognition data and where it comes from, or how it is processed. If we don't know how the data was collected, for scientific purposes, it cannot be ethical to use it. For commercial purposes, using and implementing this data would mean some people would be unfairly disadvantaged, which in that case, it is not fully ethical to use it either. The data we have for facial recognition as it currently stands is not unbiased, and Buolamwini and Gebru do a good job at pointing this out.

# Should race be used as a variable? Is it a proxy for something else? What about gender?
The question of if race should be used as a variable is a question about affirmative action or no affirmative action. Without a doubt, affirmative action has had its benefits and also negatives. It is such an important topic and both sides of the argument are heavy. If the data will be used to judge one on solely meritocratic grounds, it may be best to not use race as a variable. However, this also depends on what you call "fair", as people from disadvantaged groups would have a harder time getting to those meritocratic grounds anyways, as the real world is far from meritocratic. This decision would have to depend on the individual study, but, transparency is key in any usage of data. Regarding if race is a proxy for something else, Buolamwini and Gebru specifically talk about this. They say that labeling skin tones in certain ways diminishes the attention to detail we can give to perhaps skin colors that lie in-between labels as they would have to be put at one category or the other. Same with gender, they say:

>"All evaluated companies provided a “gender classification” feature that uses the binary sex labels of female and male. This reductionist view of gender does not adequately capture the complexities of gender or address transgender identities. The companies provide no documentation to clarify if their gender classification systems which provide sex labels are classifying gender identity or biological sex."

Therefore, it is not completely ethical to label pigmentation as certain classes of skin color. However, it is not feasible nor comparatively useful with current technology to not label either. Again, companies or data-users should do their best to be transparent about whatever they do. Buolamwini and Gebru say that the companies that they analyzed do not provide any documentation to clarify how gender was defined. This would be an unethical use of the data. Again, if companies are to do classifications, which is a question of ethics on their own, they must provide transparency on what those classifications are and how they are obtained and used; otherwise, their practices will lack ethical boundaries. Again, Buolamwini and Gebru do a good job at highlighting this.

# Presenting work in ways that empower others to make better-informed decisions
As there would be more lighter skinned individuals and males in data sets, as we know, our algorithms would be biased to pick such individuals. The presentation here is who is picked. If we are aware of the biases, and we feed that into our algorithm, perhaps it would make a better-informed decision in order to mitigate bias as much as possible. In terms of Buolamwini and Gebru's study, they present their work very diligently, using very relevant data, and citing every other study necessary as they go on. They do this in order to publish a paper that has the exact principle of empowering others (companies, humans, or AI) to make better-informed decisions. The usage of AI for humans is truly an ethical issue. Humanity has never been fully ethical, and AI will get more powerful than humans soon. If people have an ideal of making human-sourced decisions ethically, frameworks about ethics should go hand-in-hand with the AI-processed data that they use as well, not be independent from it.


# Recognizing and mitigating bias in ourselves and in the data we use
Recruiters for a role, per se, may also have biases without the usage of AI. Naturally, when interviewing candidates, they see the other person's skin color. Though they may not realize it, it is possible for them to be subconsciously biased based on their previous life experiences. This is the same with AI: AI's previous life experiences is the data that it is fed, which we have seen to be disproportionately light skinned and male filled. Buolamwini and Gebru's study does a great job at pointing this out and helps in order to recognize this bias. People have been for a long time trying not to be biased, and although it has worked in some part, it is not perfect. Perhaps, if we train our AI in such a manner that and/or modify our data set that it is done correctly and ethically, in order to produce ethics-oriented decisions; AI facial recognition may be better at mitigating bias than us one day. However, there is a long way to get there -- and this is only possible if everyone realizes that this mitigation of bias is possible if we mitigate bias in every single step: when collecting, using, modifying, training on, and reflecting upon our data and findings altogether.


# Sources

Almeida, D., Shmarko, K. & Lomas, E. The ethics of facial recognition technologies, surveillance, and accountability in an age of artificial intelligence: a comparative analysis of US, EU, and UK regulatory frameworks. AI Ethics 2, 377–387 (2022). https://doi.org/10.1007/s43681-021-00077-w.

Buolamwini, J. &amp; Gebru, T.. (2018). Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification. <i>Proceedings of the 1st Conference on Fairness, Accountability and Transparency</i>, in <i>Proceedings of Machine Learning Research</i> 81:77-91 Available from https://proceedings.mlr.press/v81/buolamwini18a.html.





