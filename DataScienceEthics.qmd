---
title: "Facial Recognition and Data Ethics Problems"
description: |
  A piece on the question of ethics for and the intented or unintended consequences of AI facial recognition being used commercially
author: Uras Uyal
date: April 16, 2025
format: html
execute: 
  warning: false
  message: false
---

# Background
Artificial Intelligence is used in every part of our daily lives now -- in circumstances in which we realize it and in those we do not. One way of rating or classifying people, often used ethically but also just as often used unethically, is via using their face, gender, and/or skin color data. Facial recognition software are used everywhere: from direct applications such as perhaps opening your phone with a Face ID or going through automatic passport check-in to modifiable applications such as your facial data being used in order to help admit you to a job or for governmental purposes. Even if the uses <i>can be</i> in good light and most facial classification software might try to use it in order to promote just and fair decisions; this does not make decisions foolproof. Many "unnoticed" issues may arise, for example, the data itself may still be unsuitable for such purposes without initial cleansing and bias considerations. 

According to Buolamwini and Gebru's study "Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification", skin and facial data is highly skewed towards those with lighter skin and significantly skewed towards those who are classified as male for many research instances. For example, according to B&G, the six-point Fitzpatrick classification system, which is dermatologists' "gold standard" when it comes to skin classification and determining risk for skin cancer is "skewed
towards lighter skin and has three categories that can be applied to people perceived as White". For example, for the dataset they used by Adience (a market research company), they used the Fitzpatrick system. Even more striking, however, is that 86.2% of the data points existing in the Adience dataset were of people in the 3 lighter skin color data set. Of course, not all data sets are like this, but a lot of them are. The example with the Adience data set goes to show that the Fitzpatrick system, which B&G already claims is more varied in classification options for light skinned individuals, also is often used skewedly with an abundance of light skinned individuals <i>per category</i> despite there being more light skinned categories in the first place!

Many data-users who do not realize bias that may exist in the data or the classifications, or do not adjust their outcomes to this bias, even if they make their AI take decisions in good light, end up having decisions which for example strongly favor lighter skinned individuals or male ones as in B&G's study.

There are some side effects to this: the abundance of those types of facial data being more abundant will make a possible algorithm more confident to choose them, or more importantly, since algorithms will be trained on past data, past usages per capita being more abundant for lighter skinned individuals or male individuals will make the algorithm think that it is more positive <i>by nature</i> to choose such individuals; amongst other reasons. This is dangerous not only in terms of making immediate decisions but also in light that an algorithm might not even be aware of the causation that makes it have this bias.Since in our current world AI is increasingly making decisions instead of humans, this danger must be noticed. 

In short, Buolamwini and Gebru's study highlights how misclassification of racial data for computer vision can be a reason for inaccuracies in fairness, for example for mixed-race or in-between skin colors individuals, highlighting that people groups' phenotypes can change over time and represents a broad spectrum. As exmplained, their study also highlights the danger of biases that are intrinsic to classification systems and also biases that may exist in the overabundance/deficiency of certain races.

Another study by Denise Almeida, Konstantin Shmarko, and Elizabeth Lomas compares the different actions for facial recognition software that is being taken in the UK, EU, and the US. They highlight that with COVID-19, facial recognition has multiplied in terms of importance. They state that for the EU and UK, although a facial recognition oriented privacy act does not exist, the EU's general data protection law, the General Data Protection Regulation (GDPR) protects local or foreign companies from using EU citizens' data in certain ways. This regulation prevents some data uses that would otherwise be covertly covered in a long <i>terms of use</i> elsewhere in the world and freely used, often harmfully. However, the authors still claim that these measures are not enough, even for the EU. On the other hand, according to the paper, a framework in the USA does not exist for facial recognition software data privacy. However, they also highlight that some action is being made. They say that in a recent California Facial Recognition Technology (FRT) Bill that was trying to be passed to enhance the use of FRT for law enforcement purposes,

>"California (e.g., Berkeley and San Francisco) have banned FRT usage. Interestingly,
the Bill received lobbying support from Microsoft, but was fercely campaigned against by Civil Rights groups, and as such, it was not passed in June 2020. This period marked
a growing sense of unease with the ethics around FRT. In the same month, IBM stated that it would cease all export sales of FRT."

However, when looked at the issue as a widespread problem, more actions should be taken all around the world in terms of data privacy. This raises question marks regarding our use for data which may have vague underlyings, both in terms of where they came from (if they are reliably traceable) or if they include any biases that we may or may not be aware of. 

As it stands, the commercial usage of facial recognition software is a very important and relevant issue because so many possibilities of unethical usage exist: most facial recognition data itself is skewed towards biasing males or lighter skinned people, as shown by B&G; the sharing of data could be vague or with malintent; and if there were biases (on purpose or not) inputted into the data. Even if someone human wanted to use the data with good intentions, it is highly improbable that the data itself that they have is 100% ethically sound (without them realizing it is so); therefore, often leading good intentions into inaccurate and dangerous results nevertheless.

Some questions or points that may come up as good discussion prompts for facial recognition AI and ethics and for Buolamwini and Gebru's study "Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification" are as follows:



# Who was measured? Are those individuals representative of the people to whom we’d like to generalize / apply the algorithm? Should we analyze data if we do not know how the data were collected?

Buolamwini and Gebru exactly highlight in their study that the individuals that are part of the data sets for most usages of facial recognition data are not representative of the people whom we'd like to generalize for, which is usually the averages for population demographics. The demographics whom we'd like to generalize for would be the general public, with as low bias or none, if at all possible, towards any group. An ideal algorithm would look at the individual and not be biased by groups (ethnic, religious, sexual, etc.) that the individual may be part of. Instead, people who were measured were mostly lighter skinned or males to a noticeable degree. Furthermore, Denise Almeida, Konstantin Shmarko, and Elizabeth Lomas also touch upon how there is a lack of transparency in big facial recognition data and where it comes from, or how it is processed. If we don't know how the data was collected, for scientific purposes, it cannot be ethical to use it, as we don't know if it accurately and without malintent represents the demographic it surveyed. For commercial purposes, using and implementing this data would mean some people could be unfairly disadvantaged, which in that case, it is not fully ethical to use it either. The data we have for facial recognition as it currently stands is not unbiased, and Buolamwini and Gebru do a good job at pointing this out.

# Should race be used as a variable? Is it a proxy for something else? What about gender?
Race most of the time need not be used as a variable. For facial recognition software, unless for circumstances I might currently miss, skin color does not need to be classified into categories: it can exist as a spectrum. Our current data analysis technology perfectly allows for us to analyze data on a spectrum without it belonging to specific classes. Race, as a variable, in most circumstances, is a proxy for a spectrum of skin colors, and makes often data users' life just a tad bit easier but on the flipside the outcomes highly inaccurate. Especially if data will be used to judge one on solely meritocratic grounds, it may be not to even use the spectrum. 

The same goes for gender. There may exist many ways to classify gender. Male/female is a proxy for a universal classification for gender since it is the most globally obvious (and often inaccurate). If we can afford to make our data set more gender-wise flexible, we should.

B&G say that labeling skin tones in certain ways diminishes the attention to detail we can give to perhaps skin colors that lie in-between labels as they would have to be put at one category or the other. Same with gender, they say:

>"All evaluated companies provided a “gender classification” feature that uses the binary sex labels of female and male. This reductionist view of gender does not adequately capture the complexities of gender or address transgender identities. The companies provide no documentation to clarify if their gender classification systems which provide sex labels are classifying gender identity or biological sex."

Therefore, it is not completely ethical to label pigmentation as certain classes of skin color, in most circumstances. The spectrum may be a better option if we can afford it. Again, regardless of how they label it, companies or data-users should do their best to be transparent about whatever they do. Buolamwini and Gebru say that the companies that they analyzed do not provide any documentation to clarify <i>how</i> gender was defined. This would be an unethical use of the data. If they were to let the data-users know how gender/race are defined, then the data would be used for what they actually mean and not universal stereotypes that may have been put anachronistically. Again, if companies are to do classifications, which is a question of ethics on their own, they must provide transparency on what those classifications are and how they are obtained and used; otherwise, their practices will lack ethical boundaries. Again, Buolamwini and Gebru do a good job at highlighting this.

# Presenting work in ways that empower others to make better-informed decisions
As there would be more lighter skinned individuals and males in data sets, as we have established in our writing so far, our algorithms would be biased to pick such individuals. The presentation here is who is picked. If we are aware of the biases, and we feed that awareness into our algorithm, perhaps it would make a better-informed decision in order to mitigate bias as much as possible. In terms of Buolamwini and Gebru's study, they present their work very diligently, using very relevant data, and citing every other study necessary as they go on. They do this in order to publish a paper that has the exact principle of empowering others (companies, humans, or AI) to make better-informed decisions. The usage of AI for humans is truly an ethical issue. Humanity has never been fully ethical, and AI will get more powerful than humans soon. If people have an ideal of making human-sourced decisions ethically, frameworks about ethics should go hand-in-hand with the AI-processed data that they use as well, not be independent from it.


# Recognizing and mitigating bias in ourselves and in the data we use
Human recruiters for a role, per se, may also have biases without the usage of AI. Naturally, when interviewing candidates, human recruiters see the other person's skin color. Though they may not realize it, it is possible for them to be subconsciously biased based on their previous life experiences. This is the same with AI: AI's previous life experiences is the data that it is fed, which we have seen to be disproportionately light skinned and male filled. Buolamwini and Gebru's study does a great job at pointing this out and helps in order to recognize this bias. People have been for a long time trying not to be biased, and although it has worked in some part, it is not perfect. Perhaps, if we train our AI in such a manner that and/or modify our data set that it is done correctly and ethically or it <i>knows what isn't</i> done correctly or ethically, in order to produce ethics-oriented decisions; AI facial recognition may be better at mitigating bias than us one day. However, there is a long way to get there -- and this is only possible if everyone realizes that this mitigation of bias is possible if we mitigate bias in every single step: when collecting, using, modifying, training on, and reflecting upon our data and findings altogether.


# Sources

Almeida, D., Shmarko, K. & Lomas, E. The ethics of facial recognition technologies, surveillance, and accountability in an age of artificial intelligence: a comparative analysis of US, EU, and UK regulatory frameworks. AI Ethics 2, 377–387 (2022). https://doi.org/10.1007/s43681-021-00077-w.

Buolamwini, J. &amp; Gebru, T.. (2018). Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification. <i>Proceedings of the 1st Conference on Fairness, Accountability and Transparency</i>, in <i>Proceedings of Machine Learning Research</i> 81:77-91 Available from https://proceedings.mlr.press/v81/buolamwini18a.html.





